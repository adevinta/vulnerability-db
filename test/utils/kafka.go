package utils

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	asyncapi "github.com/adevinta/vulnerability-db/pkg/asyncapi/kafka"
	"github.com/adevinta/vulnerability-db/pkg/notify"
	"github.com/confluentinc/confluent-kafka-go/v2/kafka"
)

// KafkaTestBroker contains the address of the local broker used for tests.
const KafkaTestBroker = "localhost:29092"

func NewKafka(topic string) (*asyncapi.Client, error) {
	return asyncapi.NewClient("", "", KafkaTestBroker, topic)
}

type topicsOpResult []kafka.TopicResult

func (t topicsOpResult) Error() kafka.ErrorCode {
	for _, res := range t {
		if res.Error.Code() != kafka.ErrNoError {
			return res.Error.Code()
		}
	}
	return kafka.ErrNoError
}

func CreateTopics(names []string) error {
	config := kafka.ConfigMap{
		"bootstrap.servers": KafkaTestBroker,
	}
	client, err := kafka.NewAdminClient(&config)
	if err != nil {
		return err
	}

	waitDuration := time.Duration(time.Second * 60)
	opTimeout := kafka.SetAdminOperationTimeout(waitDuration)

	results, err := client.DeleteTopics(context.Background(), names, opTimeout)
	if err != nil {
		return err
	}
	tResults := topicsOpResult(results)
	if tResults.Error() != kafka.ErrNoError && tResults.Error() != kafka.ErrUnknownTopicOrPart {
		return fmt.Errorf("error deleting topic %s", tResults.Error())
	}

	var topics []kafka.TopicSpecification
	for _, name := range names {
		topic := kafka.TopicSpecification{
			Topic:         name,
			NumPartitions: 1,
		}
		topics = append(topics, topic)
	}
	for {
		results, err = client.CreateTopics(context.Background(), topics, opTimeout)
		if err != nil {
			return err
		}
		tResults = topicsOpResult(results)
		if tResults.Error() == kafka.ErrNoError {
			break
		}
		if tResults.Error() == kafka.ErrTopicAlreadyExists {
			continue
		}
		return fmt.Errorf("error creating topics: %s", tResults.Error())
	}
	return nil
}

type FindingsTopicData struct {
	Payload notify.FindingNotification
	Headers map[string][]byte
}

func ReadAllFindingsTopic(topic string) ([]FindingsTopicData, error) {
	broker := KafkaTestBroker
	config := kafka.ConfigMap{
		"go.events.channel.enable": true,
		"bootstrap.servers":        broker,
		"group.id":                 "test_" + topic,
		"enable.partition.eof":     true,
		"auto.offset.reset":        "earliest",
		"enable.auto.commit":       false,
	}
	c, err := kafka.NewConsumer(&config)
	if err != nil {
		return nil, err
	}
	defer c.Close()
	if err = c.Subscribe(topic, nil); err != nil {
		return nil, err
	}

	var topicFindingsData []FindingsTopicData
Loop:
	for ev := range c.Events() {
		switch e := ev.(type) {
		case *kafka.Message:
			data := e.Value
			finding := notify.FindingNotification{}
			// The data will be empty in case the event is a tombstone.
			if len(data) > 0 {
				err = json.Unmarshal(data, &finding)
				if err != nil {
					return nil, err
				}
			}
			headers := map[string][]byte{}
			for _, v := range e.Headers {
				headers[v.Key] = v.Value
			}
			topicData := FindingsTopicData{
				Payload: finding,
				Headers: headers,
			}
			topicFindingsData = append(topicFindingsData, topicData)
			_, err := c.CommitOffsets([]kafka.TopicPartition{
				{
					Topic:     e.TopicPartition.Topic,
					Partition: e.TopicPartition.Partition,
					Offset:    e.TopicPartition.Offset + 1,
				},
			})
			if err != nil {
				return nil, err
			}
		case kafka.Error:
			return nil, e
		case kafka.PartitionEOF:
			break Loop
		default:
			return nil, fmt.Errorf("received unexpected message %v", e)
		}
	}
	return topicFindingsData, nil
}
